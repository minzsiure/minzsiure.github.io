<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Multi-region brain model simulating hippocampal-entorhinal-neocortical interactions for spatial decision-making tasks.">
    <meta name="keywords"
        content="hippocampus, grid cells, place cells, spatial decision-making, reinforcement learning, cognitive map, Vector-HaSH, neuroscience, ICML 2025">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multi-Region Brain Model</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="icon" href="static/images/favicon.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>
    <!-- removed nav bar -->
    <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://keunhong.com">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://hypernerf.github.io">
                            HyperNeRF
                        </a>
                        <a class="navbar-item" href="https://nerfies.github.io">
                            Nerfies
                        </a>
                        <a class="navbar-item" href="https://latentfusion.github.io">
                            LatentFusion
                        </a>
                        <a class="navbar-item" href="https://photoshape.github.io">
                            PhotoShape
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav> -->


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-3 publication-title">A Multi-Region Brain Model to Elucidate the Role of<br>
                            Hippocampus in Spatially Embedded Decision-Making</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://minzsiure.github.io/">Yi Xie</a><sup>1,2</sup>,</span>
                            <span class="author-block">
                                <a href="https://jd730.github.io/">Jaedong Hwang</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://brodylab.org/">Carlos Brody</a><sup>2,3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://pni.princeton.edu/people/david-tank">David Tank</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://fietelab.mit.edu/">Ila Fiete</a><sup>1</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>MIT,</span>
                            <span class="author-block"><sup>2</sup>Princeton Neuroscience Institute,</span>
                            <span class="author-block"><sup>3</sup>Howard Hughes Medical Institute</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://icml.cc/virtual/2025/poster/43834"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://icml.cc/virtual/2025/poster/43834"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <!-- <i class="ai ai-arxiv"></i> -->
                                            <img src="./static/images/biorxiv.jpg" alt="bioRxiv"
                                                style="height: 1.1em; vertical-align: middle;">
                                        </span>
                                        <span>biorXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/FieteLab/multiregion-brain-model/tree/main"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <!-- Figure of model variants -->
                    <figure style="margin-bottom: 2em;">
                        <img src="static/images/towertask.png" alt="Model variants diagram" style="width: 60%;">

                        <!-- <iframe src="static/images/towertask.pdf" width="100%" height="600px"
                            style="border: none;"></iframe> -->
                        <figcaption style="font-size: 0.9em; color: #555;">
                            Our multi-region model extends Vector-HaSH (Chandra et al., <em>Nature</em>, 2025), a
                            mechanistic model of
                            entorhinal, hippocampal, and
                            neocortical circuitry, into a reinforcement learning (RL) framework to evaluate (i)
                            behavioral performance on the task
                            and (ii) alignment of hippocampal representations with experimental data. It incorporates an
                            additional action-selection
                            RNN (representing the prefrontal cortex in abstraction) that maps the hippocampal vector
                            representation into an action.
                            The model predicts that grid cells jointly encode both self-movement velocity (a physical
                            variable) and decision
                            evidence increments (a cognitive variable).
                        </figcaption>
                    </figure>

                    <!-- Teaser paragraph -->
                    <div class="content has-text-justified" style="margin-bottom: 1.5em;">
                        <p>
                            <strong>Intelligent behavior emerges from interactions among interconnected brain
                                regions--not from isolated
                                activity.</strong> Yet most decision-making
                            studies focus on single areas. To uncover how structured <strong>multi-region</strong>
                            circuits support decision-making in
                            physical space, we build a mechanistic framework of multi-region interactions centered on
                            hippocampal interactions, with the resulting prediction now being tested
                            in neurophysiology
                            experiments with our collaborators.
                        </p>
                    </div>

                </div>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <!-- Abstract + Layman Summary Toggle -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary</h2>
                    <div class="buttons is-centered">
                        <button class="button is-small is-link is-light" onclick="showAbstract()">Abstract</button>
                        <button class="button is-small is-link is-light" onclick="showLayman()">Layman Summary</button>
                    </div>

                    <!-- Technical Abstract -->
                    <div id="abstract-text" class="content has-text-justified">
                        <p>
                            Brains excel at robust decision-making and data-efficient learning. Understanding the
                            architectures and dynamics underlying these capabilities can inform inductive biases for
                            deep learning.
                            We present a multi-region brain model that explores the normative role of structured memory
                            circuits in
                            a
                            spatially embedded binary decision-making task from neuroscience.
                        </p>
                        <p>
                            We counterfactually compare the learning performance and neural representations of
                            reinforcement
                            learning (RL)
                            agents with brain models of different interaction architectures between grid and place cells
                            in the
                            entorhinal
                            cortex and hippocampus, coupled with an action-selection cortical recurrent neural network.
                            We demonstrate that a specific architecture—where grid cells receive and jointly encode
                            self-movement
                            velocity
                            signals and decision evidence increments—optimizes learning efficiency while best
                            reproducing
                            experimental
                            observations relative to alternative architectures.
                        </p>
                        <p>
                            Our findings thus suggest brain-inspired structured architectures for efficient RL.
                            Importantly, the models make novel, testable predictions about organization and information
                            flow within
                            the
                            entorhinal-hippocampal-neocortical circuit: we predict that grid cells must conjunctively
                            encode
                            position and
                            evidence for effective spatial decision-making, directly motivating new neurophysiological
                            experiments.
                        </p>
                    </div>

                    <!-- Layman Summary -->
                    <div id="layman-text" class="content has-text-justified" style="display: none;">
                        <p>
                            Everyday choices—like deciding when to turn down a hallway—rely on two skills: knowing
                            <strong>where</strong>
                            we are and adding up <strong>clues</strong> about what to do next. Yet, how the brain merges
                            these
                            streams
                            of information across multiple brain regions to accomplish this everyday task of making
                            decisions in
                            physical
                            space remains a mystery.
                        </p>
                        <p>
                            We built a “virtual brain” inside a reinforcement‑learning agent that links three key brain
                            circuits:
                            grid cells that track location, the hippocampus that stores memories, and a small cortical
                            network that
                            picks actions. The agent practises a classic mouse task: walking down a T‑shaped maze lined
                            with visual
                            towers ("evidence") on both sides, then turning at the end toward the side with more towers
                            (“decision”).
                        </p>
                        <p>
                            After testing all the possible brain circuit designs, we found that learning was fastest—and
                            runs were
                            shortest—when each grid cell encoded both position <strong>and</strong> a running tower
                            count. This
                            “dual‑coding” design also reproduced the unusual firing patterns recorded in the real mouse
                            hippocampus
                            during the same task, whereas other designs did not.
                        </p>
                        <p>
                            For neuroscience, our results predict that biological grid cells combine “Where am I?” with
                            “How much
                            evidence have I gathered?”—a hypothesis that future neurophysiology experiments can directly
                            test. For
                            machine learning, adding such structured memory maps to learning agents can reduce training
                            demands,
                            enabling lighter, more efficient AI that makes decisions in the real world.
                        </p>
                    </div>
                </div>
            </div>

            <script>
                function showAbstract() {
                    document.getElementById("abstract-text").style.display = "block";
                    document.getElementById("layman-text").style.display = "none";
                }

                function showLayman() {
                    document.getElementById("abstract-text").style.display = "none";
                    document.getElementById("layman-text").style.display = "block";
                }
            </script>
    </section>


    <!-- ‑‑‑‑‑‑‑‑ Section: Task & Model  ‑‑‑‑‑‑‑‑ -->
    <section class="section">
        <div class="container is-max-desktop">

            <div class="columns is-centered">
                <!-- Text column (left) -->
                <div class="column is-half">
                    <div class="content">
                        <h3 class="title is-4">Accumulating‑Towers Task</h3>
                        <p>
                            In this spatial decision‑making task, an agent walks down a virtual corridor and is
                            stochastically presented with towers on the left and right. At the end of the corridor,
                            it must turn to the side with more towers—requiring the integration of evidence over
                            space and time. This task has become a shared benchmark across a growing experimental
                            and theoretical neuroscience community, enabling standardized, reproducible studies of
                            memory, integration, navigation, and decision‑making across multiple brain systems.
                        </p>
                    </div>
                </div>

                <!-- Image column (right) -->
                <div class="column is-half has-text-centered">
                    <figure style="margin-top: 1em;">
                        <img src="static/images/nieh.png"
                            alt="Schematic of the accumulating-towers task from Nieh et al., 2021"
                            style="max-width: 50%; height: auto;">
                        <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
                            Schematic of the accumulating‑towers task. Adapted from Nieh et al., <em>Nature</em>, 2021.
                        </figcaption>
                    </figure>
                </div>
            </div>


            <!--‑‑‑‑‑‑‑ Grid‑coding scheme row with side-by-side layout  ‑‑‑‑‑‑‑-->
            <div class="columns is-centered">
                <!-- Image column -->
                <div class="column is-half has-text-centered">
                    <figure style="margin-top: 1em;">
                        <img src="static/images/nonvariants.png" alt="Joint vs disjoint grid‑cell representations"
                            style="max-width: 100%; height: auto;">
                        <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
                            Example: The agent moves three positions forward, encountering evidence values +1, –1, +1.
                            In joint coding (top), each module tracks both variables; in disjoint coding (bottom),
                            modules
                            specialize.
                        </figcaption>
                    </figure>
                </div>

                <!-- Text column -->
                <div class="column is-half">
                    <div class="content">
                        <h3 class="title is-4">Grid Cell Coding Schemes</h3>
                        <p>
                            Grid cells are organized into K periodic two-dimensional one-hot modules, so the resulting
                            grid vector is K-hot.
                            While we include hypotheses that assume the conventional role of grid cells encoding
                            physical
                            position (M1 and M2), we
                            also test the hypothesis that grid cells conjunctively encode both position and accumulated
                            evidence (M3, M4, M5).
                        </p>
                        <p>
                            There are two forms of conjunctive coding:
                            In the <strong>joint coding scheme</strong>, each grid module encodes both position and
                            evidence along separate axes,
                            producing “wiggling” activation patterns on hexagonal lattices.
                            In contrast, the <strong>disjoint scheme</strong> allocates separate modules to position or
                            evidence only, so that each
                            module encodes one variable along a single axis.

                            We evaluate these coding strategies in our counterfactual models: M4 implements the disjoint
                            scheme, whereas M3 and M5
                            implement joint coding.
                        </p>
                    </div>
                </div>


            </div>

            <!-- Multi‑Region Model (M5) in its own row -->
            <div class="columns is-centered">
                <div class="column is-full">
                    <div class="content has-text-justified">
                        <h3 class="title is-4">Alternative Multi-Region Interaction
                            Hypotheses</h3>
                        <p>
                            We systematically evaluate competing hypotheses about how grid cells and the
                            entorhinal-hippocampal system
                            contribute to spatially embedded decision-making. Our counterfactual models explore whether
                            grid cells encode
                            only position (M1, M2) as following the conventional understanding, conjunctively encode
                            position and decision evidence (M3–M5), or
                            whether the hippocampus
                            receives additional sensory input from the lateral entorhinal cortex (M2, M4, M5). In the
                            case of conjunctive grid code, we also distinguish
                            between <em>joint</em>
                            integration models, in which individual grid modules encode both variables simultaneously
                            (M3, M5), and
                            <em>disjoint</em> models, in which separate grid modules encode position and evidence
                            independently (M4).
                            These architectural variations allow us to test their roles in supporting task performance
                            and hippocampal
                            coding patterns observed experimentally.
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-top: 1em;">
                        <img src="static/images/model_schematic.png" alt="model diagram"
                            style="max-width: 100%; height: auto;">
                        <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
                            Counterfactual models of hypotheses on neural code and information flow tested in our paper.
                        </figcaption>
                    </figure>
                </div>
            </div>

            <div class="columns is-centered">
                <!-- Text column on the left -->
                <div class="column is-half">
                    <div class="content has-text-justified">
                        <h3 class="title is-4">Multi‑Region Model (M5)</h3>
                        <p>
                            Our final model, <strong>M5</strong>, builds on the Vector‑HaSH architecture and
                            incorporates a cortical action‑selection RNN to learn the accumulating‑towers task via
                            reinforcement learning. M5 embodies the hypothesis that grid cells conjunctively encode
                            self‑movement velocity and decision‑evidence increments—enabling both efficient learning
                            and accurate reproduction of hippocampal activity patterns observed in vivo.
                        </p>
                    </div>
                </div>

                <!-- Image column on the right -->
                <div class="column is-half has-text-centered">
                    <figure style="margin-top: 1em;">
                        <img src="static/images/m5.png" alt="M5 diagram" style="max-width: 100%; height: auto;">
                    </figure>
                </div>
            </div>



            <!-- ‑‑‑‑‑‑‑‑ Section: Animation demo  ‑‑‑‑‑‑‑‑ -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>

                    <!--‑‑‑‑‑‑‑ Joint‑Integration result figure  ‑‑‑‑‑‑‑-->
                    <h3 class="title is-4">Joint Integration Model Induces Efficient Learning</h3>

                    <div class="content has-text-justified">
                        <p>
                            In training, only models whose grid cells <em>jointly</em> encode both
                            position and evidence (M3 & M5) achieve near-perfect success while minimizing exploration
                            time. The additional sensory projection to the hippocampus additionally induces fast
                            navigation in M5, in comparison to M3. The perfomrnace of baseline RNNs (M0, M0+) and
                            disjoint-coding variants
                            (M4) lag far behind, while the
                            models with position-only grid code don't learn the task at all (M1, M2).
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/new_rnn_baseline_black_m0plus.png"
                            alt="Training curves for model variants" style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(A)</strong> Cumulative success rate in training.
                            <strong>(B)</strong> Average steps per episode (a low value indicates fast navigation).
                        </figcaption>
                    </figure>

                    <br>

                    <!--‑‑‑‑‑‑‑ Joint grid code → hippocampal place fields  ‑‑‑‑‑‑‑-->
                    <h3 class="title is-4">Only Joint Integration Models Reproduce Experimental Hippocampal
                        Maps</h3>

                    <div class="content has-text-justified">
                        <p>
                            Conjunctive tuning of grid cells to both position and evidence (right, M5 & M3) is
                            necessary and sufficient
                            to
                            generate the hippocampal firing-field structure reported by <em>Nieh et al.</em> (2021).
                            Disjoint (middle, M4)
                            models
                            fail to capture these place‑cell sequences.
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/hpc.png" alt="Hippocampal firing field comparison"
                            style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(1)</strong> Experimental data from Nieh et al., showing hippocampal cells are tuned
                            to both
                            position (top) and
                            evidence (bottom) and exhibit choice-specific place-field sequences.
                            <strong>(2)</strong> Disjoint-grid model (M4) reproduces position and evidence fields but
                            lacks the choice-specific
                            pattern.
                            <strong>(3)</strong> Joint-grid models (M5 shown; M3 qualitatively similar) capture both
                            conjunctive firing fields
                            and the choice-specific property.
                        </figcaption>
                    </figure>

                    <h3 class="title is-4">Only Joint Integration Model With Activated EC
                        Pathway Exhibits Well-Separated Low-Dimensional
                        Co-representation of Task Variables</h3>

                    <div class="content has-text-justified">
                        <p>
                            Principal-component projections of hippocampal population activity reveal that
                            only M5 organizes task variables (position & local evidence velocity) into well-separated
                            clusters.
                            In contrast, none of the other models exhibit low-dimensional separation of task variables
                            (e.g., M4 as shown).
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/pca.png" alt="Hippocampal firing field comparison"
                            style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(A)</strong> Disjoint grid model (M4) show
                            no clear structure for position or evidence.
                            <strong>(B)</strong> Joint grid + sensory projection model
                            (M5) show cleanly low-dimensional separation in position and local evidence velocity,
                            potentially beneficial for the fast learning and navigation
                            efficiency.
                        </figcaption>
                    </figure>


                </div>
            </div> <!-- /Animation columns -->

            <!-- ‑‑‑‑‑‑‑‑ Section: Related Work  ‑‑‑‑‑‑‑‑ -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Related Work</h2>
                    <div class="content has-text-justified">
                        <p>
                            <a href="https://www.nature.com/articles/s41586-021-03652-7">Nieh et al.</a> observed
                            conjunctive coding of position and evidence in rodent hippocampus during the same task.
                        </p>
                        <p>
                            <a href="https://www.nature.com/articles/s41586-024-08392-y">Chandra et al.</a>
                            proposed Vector-HaSH, a mechanistic model for the entorhinal-hippocampal-neocortical
                            circuit, on which our model
                            builds.
                        </p>
                    </div>
                </div>
            </div> <!-- /Related Work columns -->

        </div> <!-- /container -->
    </section>
    <!-- ‑‑‑ end of fixed-width content; rest of page (BibTeX, footer) already uses containers ‑‑‑ -->

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@inproceedings{xie2025multi,
            title={A Multi-Region Brain Model to Elucidate the Role of Hippocampus in Spatially Embedded Decision-Making},
            author={Yi Xie and Jaedong Hwang and Carlos D. Brody and David W. Tank and Ila R. Fiete},
            booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
            year={2025},
            url={https://openreview.net/forum?id=sTc83mG2H9}
            }</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://icml.cc/virtual/2025/poster/43834">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/minzsiure" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Template adapted from
                            <a href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
                            Content © 2025 Eva Yi Xie and collaborators.
                            Released under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC
                                BY-SA 4.0</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>