<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Multi-region brain model simulating hippocampal-entorhinal-neocortical interactions for spatial decision-making tasks.">
    <meta name="keywords"
        content="hippocampus, grid cells, place cells, spatial decision-making, reinforcement learning, cognitive map, Vector-HaSH, neuroscience, ICML 2025">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multi-Region Brain Model</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="icon" href="static/images/favicon.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>
    <!-- removed nav bar -->
    <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://keunhong.com">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://hypernerf.github.io">
                            HyperNeRF
                        </a>
                        <a class="navbar-item" href="https://nerfies.github.io">
                            Nerfies
                        </a>
                        <a class="navbar-item" href="https://latentfusion.github.io">
                            LatentFusion
                        </a>
                        <a class="navbar-item" href="https://photoshape.github.io">
                            PhotoShape
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav> -->


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-3 publication-title">A Multi-Region Brain Model to Elucidate the Role of<br>
                            Hippocampus in Spatially Embedded Decision-Making</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://minzsiure.github.io/">Yi Xie</a><sup>1,2</sup>,</span>
                            <span class="author-block">
                                <a href="https://jd730.github.io/">Jaedong Hwang</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://brodylab.org/">Carlos Brody</a><sup>2,3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://pni.princeton.edu/people/david-tank">David Tank</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://fietelab.mit.edu/">Ila Fiete</a><sup>1</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>MIT,</span>
                            <span class="author-block"><sup>2</sup>Princeton Neuroscience Institute,</span>
                            <span class="author-block"><sup>3</sup>Howard Hughes Medical Institute</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://icml.cc/virtual/2025/poster/43834"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://icml.cc/virtual/2025/poster/43834"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <!-- <i class="ai ai-arxiv"></i> -->
                                            <img src="./static/images/biorxiv.jpg" alt="bioRxiv"
                                                style="height: 1.1em; vertical-align: middle;">
                                        </span>
                                        <span>biorXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/FieteLab/multiregion-brain-model/tree/main"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <!-- Figure of model variants -->
                    <figure style="margin-bottom: 2em;">
                        <!-- <img src="static/images/towertask.pdf" alt="Model variants diagram"> -->
                        <iframe src="static/images/towertask.pdf" width="100%" height="600px"
                            style="border: none;"></iframe>
                        <figcaption style="font-size: 0.9em; color: #555;">
                            Our multi-region model extends Vector-HaSH (Chandra et al., 2025), a mechanistic model of
                            the
                            entorhinal-hippocampal-neocortical cicuitry, into a reinforcement-learning setup, with an
                            additional action-selection RNN (prefrontal cortex in abstraction) that maps hippocampal
                            vector into an action. It predicts
                            that grid cells jointly encode both self-movement velocity (physical variable) and decision
                            evidence increment (cognitive variable).
                        </figcaption>
                    </figure>

                    <!-- Teaser paragraph -->
                    <div class="content has-text-justified" style="margin-bottom: 1.5em;">
                        <p>
                            Intelligent behavior arises not from isolated brain regions but from dynamic interactions
                            across
                            neural circuits.
                            Yet, most studies of decision-making focus on single areas in isolation, overlooking the
                            structured,
                            <strong>multi-region</strong> architecture that supports complex cognitive tasks.
                            To better understand how multiple brain regions together support decision-making in physical
                            space,
                            we model interactions across entorhinal cortex, hippocampus, and prefrontal cortex.
                            We study this by testing a reinforcement-learning (RL) agent embedded with one of <strong>a
                                series of
                                model variants</strong> that embody counterfactual
                            hypotheses of neural computation and circuit organization, ultimately leading to a final
                            model
                            (M5) with
                            the prediction that biological grid cells <strong>conjunctively encode</strong> both
                            physical
                            (position)
                            and cognitive (evidence) task variables in spatially embedded decision-making. We are
                            currently
                            verifying
                            this prediction in neurophysiological experiments with collaborators.
                        </p>
                    </div>

                </div>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <!-- Abstract + Layman Summary Toggle -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary</h2>
                    <div class="buttons is-centered">
                        <button class="button is-small is-link is-light" onclick="showAbstract()">Abstract</button>
                        <button class="button is-small is-link is-light" onclick="showLayman()">Layman Summary</button>
                    </div>

                    <!-- Technical Abstract -->
                    <div id="abstract-text" class="content has-text-justified">
                        <p>
                            Brains excel at robust decision-making and data-efficient learning. Understanding the
                            architectures and dynamics underlying these capabilities can inform inductive biases for
                            deep learning.
                            We present a multi-region brain model that explores the normative role of structured memory
                            circuits in
                            a
                            spatially embedded binary decision-making task from neuroscience.
                        </p>
                        <p>
                            We counterfactually compare the learning performance and neural representations of
                            reinforcement
                            learning (RL)
                            agents with brain models of different interaction architectures between grid and place cells
                            in the
                            entorhinal
                            cortex and hippocampus, coupled with an action-selection cortical recurrent neural network.
                            We demonstrate that a specific architecture—where grid cells receive and jointly encode
                            self-movement
                            velocity
                            signals and decision evidence increments—optimizes learning efficiency while best
                            reproducing
                            experimental
                            observations relative to alternative architectures.
                        </p>
                        <p>
                            Our findings thus suggest brain-inspired structured architectures for efficient RL.
                            Importantly, the models make novel, testable predictions about organization and information
                            flow within
                            the
                            entorhinal-hippocampal-neocortical circuit: we predict that grid cells must conjunctively
                            encode
                            position and
                            evidence for effective spatial decision-making, directly motivating new neurophysiological
                            experiments.
                        </p>
                    </div>

                    <!-- Layman Summary -->
                    <div id="layman-text" class="content has-text-justified" style="display: none;">
                        <p>
                            Everyday choices—like deciding when to turn down a hallway—rely on two skills: knowing
                            <strong>where</strong>
                            we are and adding up <strong>clues</strong> about what to do next. Yet, how the brain merges
                            these
                            streams
                            of information across multiple brain regions to accomplish this everyday task of making
                            decisions in
                            physical
                            space remains a mystery.
                        </p>
                        <p>
                            We built a “virtual brain” inside a reinforcement‑learning agent that links three key brain
                            circuits:
                            grid cells that track location, the hippocampus that stores memories, and a small cortical
                            network that
                            picks actions. The agent practises a classic mouse task: walking down a T‑shaped maze lined
                            with visual
                            towers ("evidence") on both sides, then turning at the end toward the side with more towers
                            (“decision”).
                        </p>
                        <p>
                            After testing all the possible brain circuit designs, we found that learning was fastest—and
                            runs were
                            shortest—when each grid cell encoded both position <strong>and</strong> a running tower
                            count. This
                            “dual‑coding” design also reproduced the unusual firing patterns recorded in the real mouse
                            hippocampus
                            during the same task, whereas other designs did not.
                        </p>
                        <p>
                            For neuroscience, our results predict that biological grid cells combine “Where am I?” with
                            “How much
                            evidence have I gathered?”—a hypothesis that future neurophysiology experiments can directly
                            test. For
                            machine learning, adding such structured memory maps to learning agents can reduce training
                            demands,
                            enabling lighter, more efficient AI that makes decisions in the real world.
                        </p>
                    </div>
                </div>
            </div>

            <script>
                function showAbstract() {
                    document.getElementById("abstract-text").style.display = "block";
                    document.getElementById("layman-text").style.display = "none";
                }

                function showLayman() {
                    document.getElementById("abstract-text").style.display = "none";
                    document.getElementById("layman-text").style.display = "block";
                }
            </script>
    </section>


    <!-- ‑‑‑‑‑‑‑‑ Section: Task & Model  ‑‑‑‑‑‑‑‑ -->
    <section class="section">
        <div class="container is-max-desktop">

            <div class="columns is-centered">

                <!-- Accumulating‑Towers Task -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Accumulating‑Towers Task</h2>
                        <p>
                            In this spatial decision‑making task, an agent walks down a virtual corridor and is
                            stochastically presented with towers on the left and right. At the end of the corridor,
                            it must turn to the side with more towers—requiring the integration of evidence over
                            space and time. This task has become a shared benchmark across a growing experimental
                            and theoretical neuroscience community, enabling standardized, reproducible studies of
                            memory, integration, navigation, and decision‑making across multiple brain systems.
                        </p>
                        <img src="static/images/towertask.png" alt="Accumulating‑Towers diagram">
                    </div>
                </div>

                <!-- Multi‑Region Model (M5) -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Multi‑Region Model (M5)</h2>
                        <p>
                            Our final model, <strong>M5</strong>, builds on the Vector‑HaSH architecture and
                            incorporates a cortical action‑selection RNN to learn the accumulating‑towers task via
                            reinforcement learning. M5 embodies the hypothesis that grid cells conjunctively encode
                            self‑movement velocity and decision‑evidence increments—enabling both efficient learning
                            and accurate reproduction of hippocampal activity patterns observed in vivo.
                        </p>
                        <img src="static/images/m5.png" alt="M5 diagram">
                    </div>
                </div>

            </div> <!-- /columns -->
            <!--‑‑‑‑‑‑‑ Grid‑coding scheme row with side-by-side layout  ‑‑‑‑‑‑‑-->
            <div class="columns is-centered">
                <!-- Text column -->
                <div class="column is-half">
                    <div class="content">
                        <h3 class="title is-4">Grid Cell Coding Schemes</h3>
                        <p>
                            Grid cells are organized into K periodic two-dimensional one-hot modules, so the resulting
                            grid vector is K-hot.
                            While we include hypotheses that assume the conventional role of grid cells encoding
                            physical
                            position (M1 and M2), we
                            also test the hypothesis that grid cells conjunctively encode both position and accumulated
                            evidence (M3, M4, M5).

                            There are two forms of conjunctive coding:
                            In the <strong>joint coding scheme</strong>, each grid module encodes both position and
                            evidence along separate axes,
                            producing “wiggling” activation patterns on hexagonal lattices.
                            In contrast, the <strong>disjoint scheme</strong> allocates separate modules to position or
                            evidence only, so that each
                            module encodes one variable along a single axis.

                            We evaluate these coding strategies in our counterfactual models: M4 implements the disjoint
                            scheme, whereas M3 and M5
                            implement joint coding.
                        </p>
                    </div>
                </div>

                <!-- Image column -->
                <div class="column is-half has-text-centered">
                    <figure style="margin-top: 1em;">
                        <img src="static/images/non_variants.png" alt="Joint vs disjoint grid‑cell representations"
                            style="max-width: 100%; height: auto;">
                        <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
                            Example: The agent moves three positions forward, encountering evidence values +1, –1, +1.
                            In joint coding (top), each module tracks both variables; in disjoint coding (bottom),
                            modules
                            specialize.
                        </figcaption>
                    </figure>
                </div>
            </div>


            <!-- ‑‑‑‑‑‑‑‑ Section: Animation demo  ‑‑‑‑‑‑‑‑ -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>

                    <!--‑‑‑‑‑‑‑ Joint‑Integration result figure  ‑‑‑‑‑‑‑-->
                    <h3 class="title is-4">Joint Integration Model Induces Efficient Learning</h3>

                    <div class="content has-text-justified">
                        <p>
                            In training, only models whose grid cells <em>jointly</em> encode both
                            position and evidence (M3 & M5) achieve near-perfect success while minimizing exploration
                            time. The additional sensory projection to the hippocampus additionally induces fast
                            navigation in M5, in comparison to M3. The perfomrnace of baseline RNNs (M0, M0+) and
                            disjoint-coding variants
                            (M4) lag far behind, while the
                            models with position-only grid code don't learn the task at all (M1, M2).
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/new_rnn_baseline_black_m0plus.png"
                            alt="Training curves for model variants" style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(A)</strong> Cumulative success rate in training.
                            <strong>(B)</strong> Average steps per episode (a low value indicates fast navigation).
                        </figcaption>
                    </figure>

                    <br>

                    <!--‑‑‑‑‑‑‑ Joint grid code → hippocampal place fields  ‑‑‑‑‑‑‑-->
                    <h3 class="title is-4">Only Joint Integration Models Reproduce Experimental Hippocampal
                        Maps</h3>

                    <div class="content has-text-justified">
                        <p>
                            Conjunctive tuning of grid cells to both position and evidence (right, M5 & M3) is
                            necessary and sufficient
                            to
                            generate the hippocampal firing-field structure reported by <em>Nieh et al.</em> (2021).
                            Disjoint (middle, M4)
                            models
                            fail to capture these place‑cell sequences.
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/hpc.png" alt="Hippocampal firing field comparison"
                            style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(1)</strong> Experimental data from Nieh et al., showing hippocampal cells are tuned
                            to both
                            position (top) and
                            evidence (bottom) and exhibit choice-specific place-field sequences.
                            <strong>(2)</strong> Disjoint-grid model (M4) reproduces position and evidence fields but
                            lacks the choice-specific
                            pattern.
                            <strong>(3)</strong> Joint-grid models (M5 shown; M3 qualitatively similar) capture both
                            conjunctive firing fields
                            and the choice-specific property.
                        </figcaption>
                    </figure>

                    <h3 class="title is-4">Only Joint Integration Model With Activated EC
                        Pathway Exhibits Well-Separated Low-Dimensional
                        Co-representation of Task Variables</h3>

                    <div class="content has-text-justified">
                        <p>
                            Principal-component projections of hippocampal population activity reveal that
                            only M5 organizes task variables (position & local evidence velocity) into well-separated
                            clusters.
                            In contrast, none of the other models exhibit low-dimensional separation of task variables
                            (e.g., M4 as shown).
                        </p>
                    </div>

                    <figure class="has-text-centered" style="margin-bottom:2em;">
                        <img src="static/images/pca.png" alt="Hippocampal firing field comparison"
                            style="max-width:100%; height:auto;">
                        <figcaption style="font-size:0.9em; color:#555;">
                            <strong>(A)</strong> Disjoint grid model (M4) show
                            no clear structure for position or evidence.
                            <strong>(B)</strong> Joint grid + sensory projection model
                            (M5) show cleanly low-dimensional separation in position and local evidence velocity,
                            potentially beneficial for the fast learning and navigation
                            efficiency.
                        </figcaption>
                    </figure>


                </div>
            </div> <!-- /Animation columns -->

            <!-- ‑‑‑‑‑‑‑‑ Section: Related Work  ‑‑‑‑‑‑‑‑ -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Related Work</h2>
                    <div class="content has-text-justified">
                        <p>
                            <a href="https://www.nature.com/articles/s41586-021-03652-7">Nieh et al.</a> observed
                            conjunctive coding of position and evidence in rodent hippocampus during the same task.
                        </p>
                        <p>
                            <a href="https://www.nature.com/articles/s41586-024-08392-y">Chandra et al.</a>
                            proposed Vector-HaSH, a mechanistic model for the entorhinal-hippocampal-neocortical
                            circuit, on which our model
                            builds.
                        </p>
                    </div>
                </div>
            </div> <!-- /Related Work columns -->

        </div> <!-- /container -->
    </section>
    <!-- ‑‑‑ end of fixed-width content; rest of page (BibTeX, footer) already uses containers ‑‑‑ -->

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@inproceedings{xie2025multi,
            title={A Multi-Region Brain Model to Elucidate the Role of Hippocampus in Spatially Embedded Decision-Making},
            author={Yi Xie and Jaedong Hwang and Carlos D. Brody and David W. Tank and Ila R. Fiete},
            booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
            year={2025},
            url={https://openreview.net/forum?id=sTc83mG2H9}
            }</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://icml.cc/virtual/2025/poster/43834">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/minzsiure" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Template adapted from
                            <a href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
                            Content © 2025 Eva Yi Xie and collaborators.
                            Released under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC
                                BY-SA 4.0</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>